2018-A guide to convolution arithmetic for deep learning
https://arxiv.org/pdf/1603.07285.pdf


# two_stream_net for vidio recognition
paper: [Two-Stream Convolutional Networks for Action Recognition in Vidios](https://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf)


paper: [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf)
paper: [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf)

paper: [Scalable Object Detection using Deep Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf)
paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)
paper: [DeepPose: Human Pose Estimation via Deep Neural Networks](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/42237.pdf)
paper: [Deep Neural Networks for Object Detection](https://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)

paper: [Dual Attention Network for Scene Segmentation](https://arxiv.org/pdf/1809.02983.pdf)
paper: [Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318.pdf)
paper: [FITNETS: HINTS FOR THIN DEEP NETS](https://arxiv.org/pdf/1412.6550.pdf)
paper: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()
paper: []()