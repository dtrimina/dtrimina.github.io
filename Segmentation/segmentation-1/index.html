<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>segmentation(1) -- FCN、DeconvNet | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">segmentation(1) -- FCN、DeconvNet</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">segmentation(1) -- FCN、DeconvNet</h1><div class="post-meta">Sep 11, 2019<span> | </span><span class="category"><a href="/categories/Segmentation/">Segmentation</a></span></div><div class="post-content"><p>paper: <a href="http://xxx.itp.ac.cn/pdf/1605.06211.pdf" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p>
<h5 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h5><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/Segmentation/FCN/FCN-architecture.jpg" height="70%" width="70%"></div>

<ul>
<li>首先在语义分割问题中使用end-to-end的CNN model。</li>
<li>FCN使用pretrained VGG16，将VGG16后面的全连接层用卷积层来代替，因此输入可以是不固定的尺寸。</li>
<li>最后所使用的1x1卷积进行分类，输出的channel数为num_calsses+1(背景)，本质上是一个像素级别的分类问题。</li>
</ul>
<div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/Segmentation/FCN/FCN_detail.jpg" height="90%" width="90%"></div>  

<ul>
<li><p>FCN-32s对前面预测出的结果直接Upsample到输入图像大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv7 = conv7_classifier(conv7)</span><br><span class="line">output = <span class="number">32</span>xUpsample(conv7)  <span class="comment">#得到原图大小的output</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>FCN-16s使用skip connection融合conv7和pool4的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv7 = conv7_classifier(conv7)</span><br><span class="line">conv7 = <span class="number">2</span>xUpsample(conv7)</span><br><span class="line">pool4 = pool4_classifier(pool4)</span><br><span class="line">output = <span class="number">16</span>xUpsample(conv7+pool4)  <span class="comment">#得到原图大小的output</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>FCN-8s使用类似的连接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv7 = conv7_classifier(conv7)</span><br><span class="line">conv7 = <span class="number">4</span>xUpsample(conv7)</span><br><span class="line">pool4 = pool4_classifier(pool4)</span><br><span class="line">pool4 = <span class="number">2</span>xUpsample(pool4)  </span><br><span class="line">pool3 = pool3_classifier(pool3)</span><br><span class="line">output = <span class="number">8</span>xUpsample(conv7+pool4+pool3)  <span class="comment">#得到原图大小的output</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>上述中Upsample也可以使用转置卷积，有可以学习的参数。</p>
</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>分类结果还不够精细</li>
<li>没有考虑物体与物体之间的空间特征</li>
</ul>
<hr>
<p>paper: <a href="http://xxx.itp.ac.cn/pdf/1505.04366v1" target="_blank" rel="noopener">Learning Deconvolution Network for Semantic Segmentation</a></p>
<h5 id="总体结构-1"><a href="#总体结构-1" class="headerlink" title="总体结构"></a>总体结构</h5><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/Segmentation/Deconvnet/deconvnet-arcitecture.jpg" height="90%" width="90%"></div>

<p>编码部分：</p>
<ul>
<li>使用VGG16结构，包括全连接层，这样训练就包括了全连接层的很多参数。</li>
<li>需要记录MaxPooling中的位置信息，供解码部分使用</li>
</ul>
<p><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/Segmentation/Deconvnet/unpooling-deconv.jpg" height="70%" width="70%"></div><br>解码部分：</p>
<ul>
<li>Unpooling的位置信息使用对称的Maxpooling中记录的位置信息</li>
<li>使用Unpooling+Deconvolution的组合(这也是本文的创新点)</li>
</ul>
</div><div class="tags"><a href="/tags/Segmentation/">Segmentation</a></div><div class="post-nav"><a class="pre" href="/Classification/classification-4/">Inceptions -- GoogLeNet, V2, V3, V4</a><a class="next" href="/summary/CNN-architecture-summary/">Systematic evaluation of CNN advances on the ImageNet</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/summary/" style="font-size: 15px;">summary</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-4/">segmentation(4) -- DilatedNet、DRN</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-3/">segmentation(3) -- ENet、LinkNet、FC-DenseNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-2/">segmentation(2) -- U-Net、SegNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-4/">Inceptions -- GoogLeNet, V2, V3, V4</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-1/">segmentation(1) -- FCN、DeconvNet</a></li><li class="post-list-item"><a class="post-list-link" href="/summary/CNN-architecture-summary/">Systematic evaluation of CNN advances on the ImageNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/SqueezeNet/">SqueezeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/Detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-3/">classification(3) -- VGG、Resnet、GoogLeNet、SPP-net</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>