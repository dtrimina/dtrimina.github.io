<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>classification(2) -- NIN、BatchNorm、Highway、PReLU | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">classification(2) -- NIN、BatchNorm、Highway、PReLU</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">classification(2) -- NIN、BatchNorm、Highway、PReLU</h1><div class="post-meta">May 25, 2019<span> | </span><span class="category"><a href="/categories/Classification/">Classification</a></span></div><div class="post-content"><p>paper: <a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network In Network</a>  </p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a><strong>Contributions</strong></h3><ol>
<li>mlpconv layer </li>
<li>global average pooling </li>
</ol>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a><strong>Architecture</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/NIN/NIN.jpg" height="60%" width="60%"></div>  

<p>The cross channel parametric pooling layer can be seen as <strong>a convolution layer with 1x1 convolution kernel</strong>. </p>
<h5 id="Why-using-multilayer-perceptron"><a href="#Why-using-multilayer-perceptron" class="headerlink" title="Why using multilayer perceptron ?"></a>Why using multilayer perceptron ?</h5><ul>
<li>multilayer perceptron is compatible with the structure of convolutional neural networks, which is trained using back-propagation. </li>
<li>multilayer perceptron can be a deep model itself, which is consistent with the spirit of feature re-use. </li>
</ul>
<h5 id="Why-global-average-pooling"><a href="#Why-global-average-pooling" class="headerlink" title="Why global average pooling ?"></a>Why global average pooling ?</h5><ul>
<li>it is more native to the convolution structure by enforcing correspondences between feature maps and categories. </li>
<li>there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. </li>
<li>global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.</li>
</ul>
<p>Other details are in the paper. </p>
<hr>
<p>paper: <a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>  </p>
<p><strong>internal covariate shift problem</strong>: the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.  </p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a><strong>Algorithm</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Batch_Normalization/BN.jpg" height="60%" width="60%"></div>  

<p>Normalize each scalar feature independently,by making it have the mean of zero and the variance of 1. but <strong>this may change what the layer can represent</strong>. To address this, it should make sure that the transformation inserted in the network can represent the identity transform. γ and β which can scale and shift the normalized value are to be learned.</p>
<h3 id="How-it-work"><a href="#How-it-work" class="headerlink" title="How it work"></a><strong>How it work</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Batch_Normalization/why_BN.png" height="60%" width="60%"></div>  

<ul>
<li>allowing us to use much <strong>higher learning rates</strong> without the risk of divergence can make learning faster. </li>
<li>regularizes the model and reduces the need for Dropout </li>
<li>make it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes</li>
</ul>
<h3 id="Batch-Normalized-Convolutional-Networks"><a href="#Batch-Normalized-Convolutional-Networks" class="headerlink" title="Batch-Normalized Convolutional Networks"></a><strong>Batch-Normalized Convolutional Networks</strong></h3><ul>
<li>the bias b can be ignoredsince its effect will be canceled by the subsequent mean subtraction. Thus, <code>z = g(W*u + b)</code> is replaced with <code>z = g(BN(W*u))</code> </li>
<li>suppose input feature map size is (batch_size, channel, weight, height), it’s a channel-wise normalization. </li>
</ul>
<hr>
<p>paper: <a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Networks</a>  </p>
<p>It is well known that deep networks can represent certain function classes exponentially more efficiently than shallow ones. However, network training becomes more difficult with increasing depth and training of very deep networks. Inspired by LSTM, highway networks actually utilize the gating mechanism to pass information almost unchanged through many layers.</p>
<h3 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a><strong>Architecture</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Highway_Networks/3.jpg" height="90%" width="90%"></div><br>the transform gate defined as:<br><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Highway_Networks/Tx.jpg" height="50%" width="50%"></div><br>so:<br><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Highway_Networks/4.jpg" height="50%" width="50%"></div> 

<ul>
<li>H(x) and T(x) are the same dimension. </li>
<li>use a plain layer (without highways) to change dimensionality and then continue with stacking highway layers. </li>
<li><strong>b<sub>T</sub></strong> can be initialized with a negative value (e.g. -1, -3 etc.) </li>
<li>highway networks performs better than plain networks when networks go deeper. </li>
</ul>
<hr>
<p>paper: <a href="https://arxiv.org/pdf/1502.01852.pdf" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<h3 id="Parametric-Rectified-Linear-Unit-PReLU"><a href="#Parametric-Rectified-Linear-Unit-PReLU" class="headerlink" title="Parametric Rectified Linear Unit (PReLU)"></a>Parametric Rectified Linear Unit (PReLU)</h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/PReLU-nets/PReLU.jpg" height="70%" width="70%"></div>  

<p>Above is ReLU vs. PReLU,  <em>a</em> is a coefficient controlling the slope of the negative part and a learnable parameter.  </p>
<p>two interesting phenomena:</p>
<ul>
<li>First, the first conv layer (conv1) has coefficients (0.681 and 0.596) significantly greater than 0. As the filters of conv1 are mostly Gabor-like filters such as edge or texture detectors, the learned results show that both positive and negative responses of the filters are respected.</li>
<li>Second, for the channel-wise version, the deeper conv layers in general have smaller coefficients. This implies that<br>the activations gradually become “more nonlinear” at increasing depths. In other words, the learned model tends to keep more information in earlier stages and becomes more discriminative in deeper stages.</li>
</ul>
<p>notice:</p>
<ul>
<li>do not use weight decay (l2 regularization) when updating <em>a</em>. A weight decay tends to push <em>a</em> to zero, and thus biases PReLU toward ReLU. Even without regularization, the learned coefficients rarely have a magnitude larger than 1 in our experiments.</li>
<li>do not constrain the range of <em>a</em> so that the activation function may be non-monotonic. </li>
<li>We use <em>a</em> = 0.25 as the initialization. </li>
</ul>
<h3 id="He’s-initialization-method"><a href="#He’s-initialization-method" class="headerlink" title="He’s initialization method"></a>He’s initialization method</h3><p><strong>a zero-mean Gaussian distribution whose standard deviation (std) is np.sqrt(2/n) and bias to 0.</strong></p>
</div><div class="tags"><a href="/tags/Classification/">Classification</a></div><div class="post-nav"><a class="pre" href="/Classification/SKNet/">SKNet</a><a class="next" href="/Classification/classification-1/">classification(1) -- Alexnet、ZFNet、OverFeat</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/summary/" style="font-size: 15px;">summary</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-4/">segmentation(4) -- DilatedNet、DRN</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-3/">segmentation(3) -- ENet、LinkNet、FC-DenseNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-2/">segmentation(2) -- U-Net、SegNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-4/">Inceptions -- GoogLeNet, V2, V3, V4</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-1/">segmentation(1) -- FCN、DeconvNet</a></li><li class="post-list-item"><a class="post-list-link" href="/summary/CNN-architecture-summary/">Systematic evaluation of CNN advances on the ImageNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/SqueezeNet/">SqueezeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/Detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-3/">classification(3) -- VGG、Resnet、GoogLeNet、SPP-net</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>