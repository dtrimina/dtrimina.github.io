<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>classification(1) -- Alexnet、ZFNet、OverFeat | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">classification(1) -- Alexnet、ZFNet、OverFeat</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">classification(1) -- Alexnet、ZFNet、OverFeat</h1><div class="post-meta">May 21, 2019<span> | </span><span class="category"><a href="/categories/Classification/">Classification</a></span></div><div class="post-content"><p>paper: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>We need large dataset to solve complex recognition tasks, and we need a model with a large learning capacity. Beside, model needs to have a generalization performance, not just perform well in a single dataset. Let’s start our CNN tour here.</p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a><strong>Contributions</strong></h3><ol>
<li>large cnn used in the ILSVRC-2010 and ILSVRC-2012</li>
<li>highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training CNN</li>
<li>features which improve CNN’s performance and reduce its training time</li>
<li>techniques for preventing overfitting</li>
</ol>
<h3 id="Pre-proccessing"><a href="#Pre-proccessing" class="headerlink" title="Pre-proccessing"></a><strong>Pre-proccessing</strong></h3><ol>
<li>image’s shorter side rescale to 256, then crop out the central 256x256 patch</li>
<li>subtract the mean activity over the training set from each pixel</li>
</ol>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a><strong>Architecture</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/AlexNet/AlexNet.jpg" height="80%" width="80%"><br></div><br>5 convolutional and 3 fc. LRN is used after the first and second convolutional layers.<br><br>### <strong>Features with the most important first</strong><br>1. <strong>ReLU</strong>: make learning faster<br>2. Multi GPUs training<br>    - one GPU’s(GTX 580) memory is not enough for training 1.2 million examples<br>    - GPUs are able to read from and write to one another’s memory directly without going through host machine memory<br>    - trains faster than one-GPU net and reduces top-1 and top-5 error rates by 1.7% and 1.2%<br>3. LRN(Local Response Normalization), while we do not use anymore, reduces top-1 and top-5 error rates by1.4% and 1.2%<br>4. overlapping pooling: use MaxPooling (kernel_size=3,stride=2), reduces top-1 and top-5 error rates by 0.4%and 0.3%<br><br>### <strong>Reducing Overfitting</strong><br>1. Data augmentation<br>    <em> at train time, random crop 224x224 + horizontal reflections; at test time, crop 4 corner and 1 center patches (x5) and their horizontal reflections (x2), then average the predictions made by the network’s softmax layer on the ten patches.
    </em> <a href="https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image" target="_blank" rel="noopener">Fancy PCA</a>, this scheme reduces the top-1 error rate by 1%<br>2. Dropout<br><br>### <strong>Training Details</strong><br>1. optimizer: SGD with momentum=0.9 and weight decay=0.0005<br>   <div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/AlexNet/optimizer.jpg" height="40%" width="40%"></div><br>2. initialize: a zero-mean Gaussian distribution with std=0.01, neuron biases in the second, fourth, fifth convolutional layers and fc layers with the constant 1<br>3. learning rate: use an equal learning rate for all layers, and divide the learning rate by 10 when validation error rate stops improving with the current lr<br><br>### <strong>Discussion</strong><br>Depth really is important for achieving the results, the AlexNet’s performance degrades if a single convolutional layers is removed.<br><br><br>————————————–<br><br><br>paper: <a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks</a><br><br>Larger labeled training sets, powerful GPU, better regularization strategies achieved good performance in detection, such as <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>. We want to know how and why they work. Let’s look what happened in the network.<br><br>### <strong>Contributions</strong><br><br><em> use a multi-layered deconvolutional network to visialize the input stimuli that excite individual feature maps at any layer in the model
</em> performs a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.<br>* discuss the arcitecture of the network based on AlexNet.<br><br>### <strong>Architecture and Comparion with AlexNet</strong><br><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/ZFNet/ZFNet.jpg" height="80%" width="80%"><br></div>

<p>There are many similarities between ZFNet and AlexNet in Arcitecture and training details. Preprocessing, SGD optimizer, learning rate adjustment strategy, using dropout and so on, more details can be seen in <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>. While it initilize the network’s weight to 0.01 and bias to 0. And the structure is different in the first layer with different kernel size and stride for better visialization. This change silghtly outperforms the arcitecture of AlexNet. Layers 3,4,5 are replaced with dense connections not sparse connections in 2 GPUs.</p>
<h3 id="Visualization-with-a-deconvnet"><a href="#Visualization-with-a-deconvnet" class="headerlink" title="Visualization with a deconvnet"></a><strong>Visualization with a deconvnet</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/ZFNet/deconv.jpg" height="50%" width="50%"><br></div>

<p>Here, deconvnets are not used in any learning capacity, just as a probe of an already trained connet.</p>
<p>Process: conv(learned filters) -&gt; ReLU -&gt; MaxPooling(need to save the location of the max value used in UnPooling) -&gt; UnPooling -&gt; ReLU -&gt; deconv(use transposed versions of learned conv filters, this means flipping each filter vertically and horizontally.)</p>
<p><a href="https://www.zybuluo.com/lutingting/note/459569" target="_blank" rel="noopener">This</a> and <a href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" target="_blank" rel="noopener">this</a> explain the deconve processing very well. More things about convolution is <a href="https://arxiv.org/pdf/1603.07285.pdf" target="_blank" rel="noopener">A guide to convolution arithmetic for deep learning</a>.</p>
<h3 id="Something-found-in-visialization-images-can-be-seen-in-the-paper"><a href="#Something-found-in-visialization-images-can-be-seen-in-the-paper" class="headerlink" title="Something found in visialization(images can be seen in the paper)"></a><strong>Something found in visialization(images can be seen in the paper)</strong></h3><ul>
<li>Feature Visialization: Layer 2 responds to corners and other edge/color conjunctions. Layer 3 has more complex invariances, capturing similar textures. Layer 4 shows significant variation, and is more class-specific: dog faces; bird’s legs. Layer 5 shows entire objects with significant pose variation.</li>
<li>Feature Evolution during Training: The lower layers of the model can be seen to converge within a few epochs. However,<br>the upper layers only develop develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged.</li>
<li>Occlusion Sensitivity:  the model is localizing the objects within the scene, as the probability of the correct class drops significantly when the object is occluded. When occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map.</li>
</ul>
<h3 id="Arcitecture-Analysis"><a href="#Arcitecture-Analysis" class="headerlink" title="Arcitecture Analysis"></a><strong>Arcitecture Analysis</strong></h3><ul>
<li>Removing the fully connected layers (6,7) or  two of the middle convolutional layers makes a relatively small difference to the error rate. However, removing both the middle convolution layers and the fully connected layers yields a model with only 4 layers whose performance is dramatically worse. This would suggest that the overall depth of the model is important for obtaining good performance.</li>
<li>while making network fatter(increasing the size of middle conv layers and fc layers), it improves slightly and has the risk of overfitting.</li>
<li>Using <strong>pre-trained model</strong> then fine tuning the softmax layer is much more better than model trained from beginning on the new set, shows the power of ImageNet feature extractor.</li>
</ul>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><strong>Discussion</strong></h3><ul>
<li><strong>visualize what your network learned</strong> can be used to identify problems and so obtain better results.</li>
<li>use <strong>models pre-trained</strong> in large datasets, such as ImageNet</li>
</ul>
<hr>
<p>paper: <a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</a></p>
<p>Classification, localization and detection are three computer vision tasks.The paper proposes a new integrated approach to these tasks with a singleConvNet.</p>
<h3 id="Contributions-1"><a href="#Contributions-1" class="headerlink" title="Contributions"></a><strong>Contributions</strong></h3><ol>
<li>use a single shared network to learn different tasks</li>
<li>a feature extractor called OverFeat</li>
</ol>
<h3 id="Vision-tasks-from-easy-to-hard"><a href="#Vision-tasks-from-easy-to-hard" class="headerlink" title="Vision tasks from easy to hard"></a><strong>Vision tasks from easy to hard</strong></h3><ul>
<li>classification: each image is assigned a single label corresponding to the main object in the image. Five guesses are allowed to find the correct answer. </li>
<li>localization: compare to classification, a bounding box for the predicted object must be returned with each guess.</li>
<li>detection: differs from localization in that there can be any number of objects in each image (including zero), and false positives are penalized by the mean average precision(mAP) measure.</li>
</ul>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/fast_model.jpg" height="70%" width="70%"><br></div><br><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/accurate_model.jpg" height="70%" width="70%"><br></div>

<p>Two models are provided, a fast(top) and accurate(down) one. The accurate model is more accurate than the fast one, however it requires nearly twice as many connections.<br>This model is similar to <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>, but with the following differences: (i) no contrast normalization is used; (ii) pooling regions are non-overlapping; (iii) larger 1st and 2nd layer feature maps, thanks to a smaller stride (2 instead of 4).<br>For classification, we use accurate model. There are no special details in training, but using multi-scale for classification rather than averaging a fixed set of 10 views(4 cornersandcenter, with horizontal flip). Using the entire image by densely running the network at each location and at multiple scales yields more views for voting, which increases robustness while remaining <strong>efficient with convnets</strong>. The last fc layers are replaced by 1x1 convolution operations, <strong>The entire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations exclusively</strong>. The main steps are as follows:   </p>
<div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/steps_for_classification.jpg" height="60%" width="60%"><br></div>  

<ol>
<li>As shown in the picture, suppose the size of unpooled layer 5 maps is 20x20x1024(1024 is number of channels);  </li>
<li>use 3x3 max pooling operation (non-overlapping regions), repeated 3x3 times for (∆x ,∆y) pixel offsets of {0,1,2}, then we get 3x3=9 maps with size 6x6x1024.  </li>
<li>The classifier can be seen as a box which has a fixed input size of 5x5 and produces a C-dimensional output vector for each location within the pooled maps. So 6x6 in and 2x2 out, we can get 3x3=9 maps with size 2x2xC.  </li>
<li>Then reshape the size to (2x3)x(2x3)xC=6x6xC, we get 36 outputs of classification.  </li>
<li>Then class-wise maxpooling, from 36xC to 1xC.</li>
<li>Then average the resulting C-dimensional vectors from different scales and flips and take the top-1 or top-5 elements from the mean class vector. different scales are shown as follows:  <div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/diff_scales.jpg" height="70%" width="70%"><br></div>  

</li>
</ol>
<h3 id="Localization"><a href="#Localization" class="headerlink" title="Localization"></a><strong>Localization</strong></h3><p>Using classification-trained network and fixing the feature extraction layer(1-5), we add a regression network (just like classifier) and train it with L2 loss to predict object bounding boxes. We then combine the regression predictions with the classification results at each location.<br>The regression network outputs 4 units which specify the edges of the bounding box. And use the same (∆x ,∆y) pixel offsets operatin similar to classification. Besides, the final regressor layer is class-specific, having 1000 different versions, one for each class.  </p>
<div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/localization.jpg" height="40%" width="40%"><br></div>  

<p>Then predict the set of classes in the top k for 6 scales, and conbine predictions using a greedy merge strategy (conbine boxed with high overlap ratio). The final predictionis given by taking the merged bounding boxes with maximum class scores.  </p>
<h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a><strong>Detection</strong></h3><p>Similar to classification training but need to predict background, Multiple location of an image may be trained simultaneously.</p>
</div><div class="tags"><a href="/tags/Classification/">Classification</a></div><div class="post-nav"><a class="pre" href="/Classification/classification-2/">classification(2) -- NIN、BatchNorm、Highway、PReLU</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/summary/" style="font-size: 15px;">summary</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-4/">segmentation(4) -- DilatedNet、DRN</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-3/">segmentation(3) -- ENet、LinkNet、FC-DenseNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-2/">segmentation(2) -- U-Net、SegNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-4/">Inceptions -- GoogLeNet, V2, V3, V4</a></li><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-1/">segmentation(1) -- FCN、DeconvNet</a></li><li class="post-list-item"><a class="post-list-link" href="/summary/CNN-architecture-summary/">Systematic evaluation of CNN advances on the ImageNet</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/SqueezeNet/">SqueezeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/Detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/Classification/classification-3/">classification(3) -- VGG、Resnet、GoogLeNet、SPP-net</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>