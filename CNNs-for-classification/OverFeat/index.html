<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>OverFeat | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">OverFeat</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">OverFeat</h1><div class="post-meta">May 21, 2019<span> | </span><span class="category"><a href="/categories/CNNs-for-classification/">CNNs_for_classification</a></span></div><div class="post-content"><p>paper: <a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</a></p>
<p>Classification, localization and detection are three computer vision tasks.The paper proposes a new integrated approach to these tasks with a singleConvNet.</p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a><strong>Contributions</strong></h3><ol>
<li>use a single shared network to learn different tasks</li>
<li>a feature extractor called OverFeat</li>
</ol>
<h3 id="Vision-tasks-from-easy-to-hard"><a href="#Vision-tasks-from-easy-to-hard" class="headerlink" title="Vision tasks from easy to hard"></a><strong>Vision tasks from easy to hard</strong></h3><ul>
<li>classification: each image is assigned a single label corresponding to the main object in the image. Five guesses are allowed to find the correct answer. </li>
<li>localization: compare to classification, a bounding box for the predicted object must be returned with each guess.</li>
<li>detection: differs from localization in that there can be any number of objects in each image (including zero), and false positives are penalized by the mean average precision(mAP) measure.</li>
</ul>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/fast_model.jpg" height="70%" width="70%"><br></div><br><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/accurate_model.jpg" height="70%" width="70%"><br></div>

<p>Two models are provided, a fast(top) and accurate(down) one. The accurate model is more accurate than the fast one, however it requires nearly twice as many connections.<br>This model is similar to <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>, but with the following differences: (i) no contrast normalization is used; (ii) pooling regions are non-overlapping; (iii) larger 1st and 2nd layer feature maps, thanks to a smaller stride (2 instead of 4).<br>For classification, we use accurate model. There are no special details in training, but using multi-scale for classification rather than averaging a fixed set of 10 views(4 cornersandcenter, with horizontal flip). Using the entire image by densely running the network at each location and at multiple scales yields more views for voting, which increases robustness while remaining <strong>efficient with convnets</strong>. The last fc layers are replaced by 1x1 convolution operations, <strong>The entire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations exclusively</strong>. The main steps are as follows:   </p>
<div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/steps_for_classification.jpg" height="60%" width="60%"><br></div>  

<ol>
<li>As shown in the picture, suppose the size of unpooled layer 5 maps is 20x20x1024(1024 is number of channels);  </li>
<li>use 3x3 max pooling operation (non-overlapping regions), repeated 3x3 times for (∆x ,∆y) pixel offsets of {0,1,2}, then we get 3x3=9 maps with size 6x6x1024.  </li>
<li>The classifier can be seen as a box which has a fixed input size of 5x5 and produces a C-dimensional output vector for each location within the pooled maps. So 6x6 in and 2x2 out, we can get 3x3=9 maps with size 2x2xC.  </li>
<li>Then reshape the size to (2x3)x(2x3)xC=6x6xC, we get 36 outputs of classification.  </li>
<li>Then class-wise maxpooling, from 36xC to 1xC.</li>
<li>Then average the resulting C-dimensional vectors from different scales and flips and take the top-1 or top-5 elements from the mean class vector. different scales are shown as follows:  <div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/diff_scales.jpg" height="70%" width="70%"><br></div>  

</li>
</ol>
<h3 id="Localization"><a href="#Localization" class="headerlink" title="Localization"></a><strong>Localization</strong></h3><p>Using classification-trained network and fixing the feature extraction layer(1-5), we add a regression network (just like classifier) and train it with L2 loss to predict object bounding boxes. We then combine the regression predictions with the classification results at each location.<br>The regression network outputs 4 units which specify the edges of the bounding box. And use the same (∆x ,∆y) pixel offsets operatin similar to classification. Besides, the final regressor layer is class-specific, having 1000 different versions, one for each class.  </p>
<div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/OverFeat/localization.jpg" height="40%" width="40%"><br></div>  

<p>Then predict the set of classes in the top k for 6 scales, and conbine predictions using a greedy merge strategy (conbine boxed with high overlap ratio). The final predictionis given by taking the merged bounding boxes with maximum class scores.  </p>
<h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a><strong>Detection</strong></h3><p>Similar to classification training but need to predict background, Multiple location of an image may be trained simultaneously.</p>
</div><div class="tags"><a href="/tags/CNNs-for-detection/">CNNs_for_detection</a><a href="/tags/CNNs-for-classification/">CNNs_for_classification</a><a href="/tags/CNNs-for-localization/">CNNs_for_localization</a></div><div class="post-nav"><a class="pre" href="/CNNs-for-classification/NIN/">NIN</a><a class="next" href="/CNNs-for-classification/ZFNet/">ZFNet</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CNNs-for-detection/" style="font-size: 15px;">CNNs_for_detection</a> <a href="/tags/CNNs-for-classification/" style="font-size: 15px;">CNNs_for_classification</a> <a href="/tags/CNNs-for-localization/" style="font-size: 15px;">CNNs_for_localization</a> <a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Resnet/">Resnet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SPP-net/">SPP-net</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/PReLU-nets/">PReLU-nets</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Highway-Networks/">Highway Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Batch-Normalization/">Batch-Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SKNet/">SKNet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/GoogLeNet/">GoogLeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/VGG/">VGG</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>