<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>ZFNet | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">ZFNet</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">ZFNet</h1><div class="post-meta">May 16, 2019<span> | </span><span class="category"><a href="/categories/CNNs-for-classification/">CNNs_for_classification</a></span></div><div class="post-content"><p>paper: <a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks</a></p>
<p>Larger labeled training sets, powerful GPU, better regularization strategies achieved good performance in detection, such as <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>. We want to know how and why they work. Let’s look what happened in the network.</p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a><strong>Contributions</strong></h3><ul>
<li>use a multi-layered deconvolutional network to visialize the input stimuli that excite individual feature maps at any layer in the model</li>
<li>performs a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.</li>
<li>discuss the arcitecture of the network based on AlexNet.</li>
</ul>
<h3 id="Architecture-and-Comparion-with-AlexNet"><a href="#Architecture-and-Comparion-with-AlexNet" class="headerlink" title="Architecture and Comparion with AlexNet"></a><strong>Architecture and Comparion with AlexNet</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/ZFNet/ZFNet.jpg" height="80%" width="80%"><br></div>

<p>There are many similarities between ZFNet and AlexNet in Arcitecture and training details. Preprocessing, SGD optimizer, learning rate adjustment strategy, using dropout and so on, more details can be seen in <a href="https://blog.dtrimina.cn/CNNs-for-image-classification/AlexNet/">AlexNet</a>. While it initilize the network’s weight to 0.01 and bias to 0. And the structure is different in the first layer with different kernel size and stride for better visialization. This change silghtly outperforms the arcitecture of AlexNet. Layers 3,4,5 are replaced with dense connections not sparse connections in 2 GPUs.</p>
<h3 id="Visualization-with-a-deconvnet"><a href="#Visualization-with-a-deconvnet" class="headerlink" title="Visualization with a deconvnet"></a><strong>Visualization with a deconvnet</strong></h3><div align="center"><br><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/ZFNet/deconv.jpg" height="50%" width="50%"><br></div>

<p>Here, deconvnets are not used in any learning capacity, just as a probe of an already trained connet.</p>
<p>Process: conv(learned filters) -&gt; ReLU -&gt; MaxPooling(need to save the location of the max value used in UnPooling) -&gt; UnPooling -&gt; ReLU -&gt; deconv(use transposed versions of learned conv filters, this means flipping each filter vertically and horizontally.)</p>
<p><a href="https://www.zybuluo.com/lutingting/note/459569" target="_blank" rel="noopener">This</a> and <a href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" target="_blank" rel="noopener">this</a> explain the deconve processing very well. More things about convolution is <a href="https://arxiv.org/pdf/1603.07285.pdf" target="_blank" rel="noopener">A guide to convolution arithmetic for deep learning</a>.</p>
<h3 id="Something-found-in-visialization-images-can-be-seen-in-the-paper"><a href="#Something-found-in-visialization-images-can-be-seen-in-the-paper" class="headerlink" title="Something found in visialization(images can be seen in the paper)"></a><strong>Something found in visialization(images can be seen in the paper)</strong></h3><ul>
<li>Feature Visialization: Layer 2 responds to corners and other edge/color conjunctions. Layer 3 has more complex invariances, capturing similar textures. Layer 4 shows significant variation, and is more class-specific: dog faces; bird’s legs. Layer 5 shows entire objects with significant pose variation.</li>
<li>Feature Evolution during Training: The lower layers of the model can be seen to converge within a few epochs. However,<br>the upper layers only develop develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged.</li>
<li>Occlusion Sensitivity:  the model is localizing the objects within the scene, as the probability of the correct class drops significantly when the object is occluded. When occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map.</li>
</ul>
<h3 id="Arcitecture-Analysis"><a href="#Arcitecture-Analysis" class="headerlink" title="Arcitecture Analysis"></a><strong>Arcitecture Analysis</strong></h3><ul>
<li>Removing the fully connected layers (6,7) or  two of the middle convolutional layers makes a relatively small difference to the error rate. However, removing both the middle convolution layers and the fully connected layers yields a model with only 4 layers whose performance is dramatically worse. This would suggest that the overall depth of the model is important for obtaining good performance.</li>
<li>while making network fatter(increasing the size of middle conv layers and fc layers), it improves slightly and has the risk of overfitting.</li>
<li>Using <strong>pre-trained model</strong> then fine tuning the softmax layer is much more better than model trained from beginning on the new set, shows the power of ImageNet feature extractor.</li>
</ul>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><strong>Discussion</strong></h3><ul>
<li><strong>visualize what your network learned</strong> can be used to identify problems and so obtain better results.</li>
<li>use <strong>models pre-trained</strong> in large datasets, such as ImageNet</li>
</ul>
</div><div class="tags"><a href="/tags/CNNs-for-classification/">CNNs_for_classification</a></div><div class="post-nav"><a class="pre" href="/CNNs-for-classification/OverFeat/">OverFeat</a><a class="next" href="/CNNs-for-classification/AlexNet/">AlexNet</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CNNs-for-classification/" style="font-size: 15px;">CNNs_for_classification</a> <a href="/tags/CNNs-for-localization/" style="font-size: 15px;">CNNs_for_localization</a> <a href="/tags/CNNs-for-detection/" style="font-size: 15px;">CNNs_for_detection</a> <a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Resnet/">Resnet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SPP-net/">SPP-net</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/PReLU-nets/">PReLU-nets</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Highway-Networks/">Highway Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Batch-Normalization/">Batch-Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SKNet/">SKNet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/GoogLeNet/">GoogLeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/VGG/">VGG</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>