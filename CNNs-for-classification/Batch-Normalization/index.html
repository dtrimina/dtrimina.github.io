<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learning CNNs and deep learning"><title>Batch-Normalization | LXY</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Batch-Normalization</h1><a id="logo" href="/.">LXY</a><p class="description">deep learning tutorial</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Batch-Normalization</h1><div class="post-meta">May 30, 2019<span> | </span><span class="category"><a href="/categories/CNNs-for-classification/">CNNs_for_classification</a></span></div><div class="post-content"><p>paper: <a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>  </p>
<p><strong>internal covariate shift problem</strong>: the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.  </p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a><strong>Algorithm</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Batch_Normalization/BN.jpg" height="60%" width="60%"></div>  

<p>Normalize each scalar feature independently,by making it have the mean of zero and the variance of 1. but <strong>this may change what the layer can represent</strong>. To address this, it should make sure that the transformation inserted in the network can represent the identity transform. γ and β which can scale and shift the normalized value are to be learned.</p>
<h3 id="How-it-work"><a href="#How-it-work" class="headerlink" title="How it work"></a><strong>How it work</strong></h3><div align="center"><img src="https://saveimages.oss-cn-hangzhou.aliyuncs.com/CNNs_for_image_classification/Batch_Normalization/why_BN.png" height="60%" width="60%"></div>  

<ul>
<li>allowing us to use much <strong>higher learning rates</strong> without the risk of divergence can make learning faster. </li>
<li>regularizes the model and reduces the need for Dropout </li>
<li>make it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes</li>
</ul>
<h3 id="Batch-Normalized-Convolutional-Networks"><a href="#Batch-Normalized-Convolutional-Networks" class="headerlink" title="Batch-Normalized Convolutional Networks"></a><strong>Batch-Normalized Convolutional Networks</strong></h3><ul>
<li>the bias b can be ignoredsince its effect will be canceled by the subsequent mean subtraction. Thus, <code>z = g(W*u + b)</code> is replaced with <code>z = g(BN(W*u))</code> </li>
<li>suppose input feature map size is (batch_size, channel, weight, height), it’s a channel-wise normalization. </li>
</ul>
</div><div class="tags"><a href="/tags/CNNs-for-classification/">CNNs_for_classification</a></div><div class="post-nav"><a class="pre" href="/CNNs-for-classification/Highway-Networks/">Highway Networks</a><a class="next" href="/CNNs-for-classification/SKNet/">SKNet</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://blog.dtrimina.cn"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CNNs-for-detection/" style="font-size: 15px;">CNNs_for_detection</a> <a href="/tags/CNNs-for-classification/" style="font-size: 15px;">CNNs_for_classification</a> <a href="/tags/CNNs-for-localization/" style="font-size: 15px;">CNNs_for_localization</a> <a href="/tags/summary/" style="font-size: 15px;">summary</a> <a href="/tags/deep-learning-tools/" style="font-size: 15px;">deep-learning-tools</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Segmentation/segmentation-1/">segmentation(1) -- FCN、DeconvNet</a></li><li class="post-list-item"><a class="post-list-link" href="/summary/CNN-architecture-summary/">Systematic evaluation of CNN advances on the ImageNet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SqueezeNet/">SqueezeNet</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-learning-tools/Ubuntu18-04-deep-learning/">Ubuntu18.04 with deep learning (cuda10.0 + pytorch1.1 + tensorflow2.0.0-beta + mxnet)</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Resnet/">Resnet</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-detection/Fast-RCNN/">Fast R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/SPP-net/">SPP-net</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/PReLU-nets/">PReLU-nets</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Highway-Networks/">Highway Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/CNNs-for-classification/Batch-Normalization/">Batch-Normalization</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">LXY.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>